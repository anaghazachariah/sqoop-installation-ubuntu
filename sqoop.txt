
Before using sqoop
--------------------------
su - hdoop
cd hadoop-3.2.1/sbin
./start-all.sh


After using sqoop
-----------------
#TYPE FOLLOWING URL IN BROWSER
     http://localhost:9870/

     click on 'utilities' in the panel->click on 'browse the file system' ->Then you can see list of tables imported->
     ->select the imported table by clicking on the target directory name in 'Name' column->download second file to view csv file


SQOOP import
************
--connect            : JDBC URL to database
--username           : username for connecting to database
--password           : password for connecting to database
--table              : name of the table to transfer
--target-dir         : specify  the  directory  on HDFS where Sqoop should import  data
--warehouse-dir      : To specify parent directory
--where              : express condition
--P                  : hidden password
--password-file      : To  load  the  password from any specified file
--as-sequencefile    : To use SequenceFile file format 
-as-avrodatafile     : To use avro file format 
--compress           : To compress file
--compression-codec  : To choose compression format
--direct             : To speedup the transfer by using native utilities provided by the database vendor for  retrieving  data from the database server or moving data back
--map-column-java    : To override the mapping
---num-mappers       : To specify number of number of mappers
--null-string        : To replace null value for text-based columns that are defined with type VARCHAR, CHAR, NCHAR, TEXT, and a few others
--null-non-string    : To replace null value for all columns other than text-based columns 

Consider example 
    #company-name of database
    #user- username of database
    #06August!- password of database
    #Employee- table name
    #/home/sqoop -give target directory

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee           
    #Sqoop will create a directory with the same name as the imported table inside your home directory on HDFS and import all data there.

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp
    #Sqoop will create a directory with the same name as the imported table inside target directory on HDFS and import all data there.Sqoop  will  reject  importing data when the final output directory already exists

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --warehouse-dir /home/sqoop
    #We have to select only parent directory.Sqoop will create a directory with the same name as the table inside the parent directory and import data there.Sqoop  will  reject  importing multiple tables with same name in the same parent directory

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --warehouse-dir /home/sqoop --where "id"=1
    #import table with entries which satisfies the condition(here "id" =1) 

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --P --table Employee --warehouse-dir /home/sqoop
    #when we run the command,sqoop will ask for password.The password will not be displayed

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password-file /home/hdoop/sqoop.password --table Employee --target-dir /home/sqoop/emp
    # load  the  password from  a given file and import data.Here the password is stored in /home/hdoop/sqoop.password
    #use the following commands to create a file for storing password
        
        $echo -n "16August!" > "sqoop.password"                                 
        $hadoop dfs -put sqoop.password /home/hdoop/sqoop.password      #storing the file inside  home directory 
        $hadoop dfs -chown 400 /home/hdoop/sqoop.password               #set the file’s permissions to 400
        $rm sqoop.password

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --as-sequencefile
    # To import data into a sequence file format

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --as-avrodatafile
    # To import data into avro file format

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --compress
    #Decrease  the  overall  size  occupied  on  HDFS  by  using  compression  for generated files.By default output files will be compressed using the GZip codec, and all files will end up with a .gz extension.

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --compress --compression-codec org.apache.hadoop.io.compress.BZip2Codec
    #Output files will be compressed using the  BZip2 codec, and all files will end up with a .bz2 extension.

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec
    #Output files will be compressed using the  snappy codec, and all files will end up with a .snappy extension.

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --direct 
    #To speed up data transfer

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --map-column-java salary=Float,age=String,address=String 
    #Changes mapping of salary to float,age and address to string.

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --num-mappers 1
    #Output files will be compressed using the  snappy codec, and all files will end up with a .snappy extension.

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --null-string '\\N'
    #Replaces null values for text based data and import.Here null values are replaced by /N

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --null-non-string '\\N'
    #Replaces null values for data other than text based data and import.Here null values are replaced by /N

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --incremental append --check-column id --last-value 1
    #specify the column containing the row’s id with --check-column. the rows with value id greater than or equal to  --last-value for  -check-column will be selected.(initially we import a table with --check-column id --last-value 0.It will have all rows in the table .assume the table undergoes some addition of rows.we can import  those new rows to our old table by giving value of --check-column of last row in the already imported table as --last-value)

$sqoop import --connect jdbc:mysql//localhost:3306/company --username zakku --password 16August! --table Employee --target-dir /home/sqoop/emp --incremental lastmodified --check-column last_update_date --last-value "2013-05-22 01:01:01"
    #The incremental mode lastmodified requires a column holding a date value containing information as to when each row was last updated. Sqoop will import only those rows that were updated after the last import. Any row that does not have a modified column, as specified in the --check-column parameter, won’t be imported.

$sqoop job --create visits -- import --connect jdbc:mysql://localhost:3306/zakku --username zakku --password 16August! --table employee --target-dir /home/sqoop/emp --incremental append --check-column id --last-value 1
    #To preserve last imported value 
    #Inorder to work job create sqoop-site.xml in sqoop-1.4.7/conf and add the following lines(if we are changing user or databse we have to change this file).Here visits is the job name
                    <?xml version="1.0" encoding="UTF-8"?>
                    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
                    <configuration>
                    <property>
                        <name>sqoop.metastore.client.autoconnect.url</name>
                        <value>jdbc:mysql://localhost:3306/zakku</value>
                    </property>
                    <property>
                        <name>sqoop.metastore.client.autoconnect.username</name>
                        <value>zakku</value>
                    </property>
                    <property>
                        <name>sqoop.metastore.client.autoconnect.password</name>
                        <value>16August!</value>
                    </property>
                    </configuration>
    #To run job(no need to run previous command everytime to import job)
            $sqoop job --exec visits

$sqoop job --list
    #To list all jobs

$sqoop job --delete visits
    #To delete jobs.Here visits getting deleted

$sqoop job --show visits
    #To see all jobs

$sqoop job --create visits -- import --connect jdbc:mysql://localhost:3306/zakku --username zakku --password-file /home/hdoop/sqoop.password --table employee --target-dir /home/sqoop/emp --incremental append --check-column id --last-value 1
    #to run sqoop job without password providing during execution

#another way to run sqoop job without password providing during execution.add following lines in sqoop-site.xml in sqoop-1.4.7/conf
                    <property>
                    <name>sqoop.metastore.client.record.password</name>
                    <value>true</value>
                    </property>


$sqoop job --exec visits -- --verbose
    #adding new parameter at runtime.here we add --verbose to visits.verbose is used to get more insight into what the job is doing



SQOOP import-all-tables
************

$sqoop import-all-tables --connect jdbc:mysql://localhost:3306/zakku --username zakku --password 16August! --warehouse-dir /home/sqoop/emp 
    #Import all tables from database.You can’t use the parameter --target-dir, as that would instruct Sqoop to import all tables into the same directory.

$sqoop import-all-tables --connect jdbc:mysql://localhost:3306/zakku --username zakku --password 16August! --exclude-tables manager,sales --warehouse-dir /home/sqoop/emp 
    #Import all tables from database excluding specific tables.Here we are excluding manager and sales tables



EXTRA INFO
**********

$hadoop fs -cat /home/sqoop/part-m-*
    #To view imported file.here /home/sqoop is the path of target-directory.Add /part-m-* to the path

$hadoop fs -rm -R /home/hdoop/file1
    #To delete hdoop file.Here /home/hdoop/file1 is the path of file to be deleted

SQOOP EXPORT
**************
 $sqoop export --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities --export-dir /home/sqoop/emp 
    #exporting hadoop table to mysql.Table name should be the destination table in mysql.Table should be existing(can be empty).--export-dir says about from which dir we have to export
    #here we are exporting table stored in dir /home/sqoop/emp as cities table in mysql.To export table stores in /home/sqoop/emp and table  cities should have same attributes

 $sqoop export --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities --export-dir /home/sqoop/emp --batch
    #Use batch mode for exporting hadoop table 

 $sqoop export -Dsqoop.export.records.per.statement=10  --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities --export-dir /home/sqoop/emp --batch
    #sqoop.export.records.per.statement  to specifices the number of records that will be used in each insert statement.here we have 10 rows inside one single insert statement

 $sqoop export -Dsqoop.export.statements.per.transaction=10  --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities --export-dir /home/sqoop/emp --batch
    #alue specified in sqoop.export.statements.per.transaction  determines  how  many  insert  statements  will  be issued  on  the  database  prior  to  committing  the  transaction  and  starting  a  new  one

 $sqoop export --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities --staging-table staging_cities --export-dir /home/sqoop/emp 
    #staging table is used to first load data to a temporary table before making changes to the real table. The staging table name is specified via the --staging-table parameter.staging table should be existing and empty

 $sqoop export --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities --staging-table staging_cities --export-dir /home/sqoop/emp 
    #--clear-staging-table is used to clear staging table before using

 $sqoop export --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities  --export-dir /home/sqoop/emp --update-key id
    #update any changed rows in msql table.here rows are identified by column id

 $sqoop export --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities  --export-dir /home/sqoop/emp --update-key id --update-mode allowinsert
    #update citites  according to /home/sqoop/emp  and inset new rows to cities if there is any in /home/sqoop/emp

 $sqoop export --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities --columns "country,city,fname,id,lname" --export-dir /home/sqoop/emp 
    #assume the order of columns in myswl is id,fname,lname,city,county..but in hdoop it is country,city,fname,lname,id(check hadoop table). then sppecify order of hdoop using --columns.column names can be given with or without ""
 $sqoop export --connect jdbc:mysql//localhost:3306/zakku --username zakku --password 16August! --table cities --columns country,city,fname,id  --export-dir /home/sqoop/emp 
    #update only given columns.we should give all columns until primary column(the order of column based on hadoop).we can remove unwanted columns after that